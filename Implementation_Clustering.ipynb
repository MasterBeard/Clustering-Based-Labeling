{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx3dI8PxjJA6WJqPsWMkJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MasterBeard/Clustering-Based-Labeling/blob/main/Implementation_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT1BDv5XG5Ze",
        "outputId": "efb031ba-3e80-457f-ca0f-3367ab60bdd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 4x4 matrices shape: (24336, 4, 5, 1)\n",
            "Train 4x5 matrices shape: (24336, 10)\n",
            "Train labels shape: (24336,)\n",
            "Validation 4x4 matrices shape: (12363, 4, 5, 1)\n",
            "Validation 4x5 matrices shape: (12363, 10)\n",
            "Validation labels shape: (12363,)\n",
            "Test 4x4 matrices shape: (11993, 4, 5, 1)\n",
            "Test 4x5 matrices shape: (11993, 10)\n",
            "Test labels shape: (11993,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy.polynomial.polynomial as poly\n",
        "\n",
        "# Define index tickers\n",
        "index_tickers = {\n",
        "    'SPX': '^GSPC',     # S&P 500\n",
        "    'IXIC': '^IXIC',    # NASDAQ Composite\n",
        "    'HSI': '^HSI',      # Hang Seng Index\n",
        "    'DJI': '^DJI',      # Dow Jones Industrial Average\n",
        "    'FCHI': '^FCHI',    # CAC 40\n",
        "    'DAXI': '^GDAXI',   # DAX\n",
        "    'N225': '^N225',    # Nikkei 225\n",
        "    'KS11': '^KS11',    # KOSPI\n",
        "    'SENSEX': '^BSESN', # BSE Sensex\n",
        "    'STOXX50': '^STOXX50E'  # EURO STOXX 50\n",
        "}\n",
        "\n",
        "# Define date ranges\n",
        "date_ranges = {\n",
        "    'train': (\"2005-01-01\", \"2015-01-01\"),\n",
        "    'val': (\"2015-01-02\", \"2019-12-31\"),\n",
        "    'test': (\"2020-01-01\", \"2024-10-31\")\n",
        "}\n",
        "\n",
        "# Store matrices and labels for each data split\n",
        "data_splits = {split: {'matrices_4x4': [], 'matrices_1st': [], 'labels3': []} for split in date_ranges}\n",
        "\n",
        "# Window size\n",
        "window_size = 11\n",
        "\n",
        "# Retrieve and process data for each time range\n",
        "for split, (start_date, end_date) in date_ranges.items():\n",
        "    # Download index data within the specified time range\n",
        "    index_data = {name: yf.download(ticker, start=start_date, end=end_date) for name, ticker in index_tickers.items()}\n",
        "\n",
        "    # Create first-order and second-order derivative matrices for each index\n",
        "    for index_name, data in index_data.items():\n",
        "        # Extract required 'Open' and 'Close' data\n",
        "        open_values = data['Open'].dropna().values\n",
        "        close_values = data['Close'].dropna().values\n",
        "\n",
        "        # Construct windowed matrices\n",
        "        for start in range(len(data) - window_size + 1):\n",
        "            # Extract each row of data\n",
        "            open_row = open_values[start:start + window_size]\n",
        "            close_row = close_values[start:start + window_size]\n",
        "\n",
        "            # Normalize using the second last value in close_row\n",
        "            normalization_factor = close_row[-2]\n",
        "            open_row = open_row / normalization_factor\n",
        "            close_row = close_row / normalization_factor\n",
        "\n",
        "            # Create interleaved array\n",
        "            combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "\n",
        "            matrix_4x4 = combined[:-2].reshape(4, 5, 1)\n",
        "            matrix_4x5 = combined[-10:].reshape(-1)\n",
        "\n",
        "            data_splits[split]['matrices_4x4'].append(matrix_4x4)\n",
        "            data_splits[split]['matrices_1st'].append(matrix_4x5)\n",
        "\n",
        "            # Create label\n",
        "            label3 = 1 if close_row[-1] > close_row[-2] else 0\n",
        "            data_splits[split]['labels3'].append(label3)\n",
        "\n",
        "# Convert each split's matrices into NumPy arrays\n",
        "train_matrices_4x4 = np.array(data_splits['train']['matrices_4x4'])\n",
        "train_matrices_4x5 = np.array(data_splits['train']['matrices_1st'])\n",
        "train_labels3 = np.array(data_splits['train']['labels3'])\n",
        "\n",
        "val_matrices_4x4 = np.array(data_splits['val']['matrices_4x4'])\n",
        "val_matrices_4x5 = np.array(data_splits['val']['matrices_1st'])\n",
        "val_labels3 = np.array(data_splits['val']['labels3'])\n",
        "\n",
        "test_matrices_4x4 = np.array(data_splits['test']['matrices_4x4'])\n",
        "test_matrices_4x5 = np.array(data_splits['test']['matrices_1st'])\n",
        "test_labels3 = np.array(data_splits['test']['labels3'])\n",
        "\n",
        "# Print the shape of each set to check the results\n",
        "print(f\"Train 4x4 matrices shape: {train_matrices_4x4.shape}\")\n",
        "print(f\"Train 4x5 matrices shape: {train_matrices_4x5.shape}\")\n",
        "print(f\"Train labels shape: {train_labels3.shape}\")\n",
        "\n",
        "print(f\"Validation 4x4 matrices shape: {val_matrices_4x4.shape}\")\n",
        "print(f\"Validation 4x5 matrices shape: {val_matrices_4x5.shape}\")\n",
        "print(f\"Validation labels shape: {val_labels3.shape}\")\n",
        "\n",
        "print(f\"Test 4x4 matrices shape: {test_matrices_4x4.shape}\")\n",
        "print(f\"Test 4x5 matrices shape: {test_matrices_4x5.shape}\")\n",
        "print(f\"Test labels shape: {test_labels3.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the number of clusters (can be adjusted as needed)\n",
        "n_clusters = 15\n",
        "\n",
        "# Initialize the KMeans model and fit it on the training set\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "train_clusters = kmeans.fit_predict(train_matrices_4x5)\n",
        "\n",
        "# Predict clusters for the validation and test sets\n",
        "val_clusters = kmeans.predict(val_matrices_4x5)\n",
        "test_clusters = kmeans.predict(test_matrices_4x5)\n",
        "\n",
        "# Get cluster centers\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "# Sort the cluster centers based on the last value of each center\n",
        "sorted_indices = np.argsort(cluster_centers[:, -1])  # Get the sorted indices\n",
        "sorted_cluster_centers = cluster_centers[sorted_indices]  # Reorder the centers using the sorted indices\n",
        "\n",
        "# Print the sorted cluster centers and their corresponding cluster IDs\n",
        "print(\"Sorted Cluster Centers (Centroids) based on the last value:\")\n",
        "for idx, center in zip(sorted_indices, sorted_cluster_centers):\n",
        "    print(f\"Cluster {idx}: {center}\")\n",
        "\n",
        "# Check the sample count distribution across clusters\n",
        "print(\"\\nTrain Cluster Distribution:\")\n",
        "unique_train, counts_train = np.unique(train_clusters, return_counts=True)\n",
        "for cluster, count in zip(unique_train, counts_train):\n",
        "    print(f\"Cluster {cluster}: {count} samples\")\n",
        "\n",
        "print(\"\\nValidation Cluster Distribution:\")\n",
        "unique_val, counts_val = np.unique(val_clusters, return_counts=True)\n",
        "for cluster, count in zip(unique_val, counts_val):\n",
        "    print(f\"Cluster {cluster}: {count} samples\")\n",
        "\n",
        "print(\"\\nTest Cluster Distribution:\")\n",
        "unique_test, counts_test = np.unique(test_clusters, return_counts=True)\n",
        "for cluster, count in zip(unique_test, counts_test):\n",
        "    print(f\"Cluster {cluster}: {count} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsAK1kVnHPaU",
        "outputId": "c8966382-278e-46ab-ff90-8834ff653468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted Cluster Centers (Centroids) based on the last value:\n",
            "Cluster 13: [1.08971399 1.08110617 1.07636711 1.05882452 1.05122376 1.02779489\n",
            " 1.01732592 1.         0.99164039 0.97368976]\n",
            "Cluster 1: [0.99174393 0.99285541 0.99411333 0.99669753 0.9978181  0.9996557\n",
            " 1.00022922 1.         0.99538042 0.9838447 ]\n",
            "Cluster 10: [1.01671224 1.01374226 1.01191719 1.00502276 1.00322311 0.99936707\n",
            " 0.99935775 1.         0.9958884  0.98514452]\n",
            "Cluster 4: [0.92983071 0.9222671  0.92447353 0.93142844 0.9381301  0.95989943\n",
            " 0.97171763 1.         1.00025388 0.99729041]\n",
            "Cluster 5: [1.01362832 1.01466222 1.01610179 1.01884694 1.01894349 1.01597453\n",
            " 1.01302796 1.         0.99968528 0.99857935]\n",
            "Cluster 0: [1.04020315 1.03856483 1.03808774 1.03498569 1.03309469 1.02362124\n",
            " 1.01821131 1.         1.00021898 0.99918126]\n",
            "Cluster 12: [0.95428166 0.95455303 0.95737509 0.96515371 0.97026989 0.98235663\n",
            " 0.98786494 1.         0.99989673 0.9998512 ]\n",
            "Cluster 8: [0.97249402 0.97466675 0.97666833 0.98155529 0.98448479 0.99116533\n",
            " 0.99403402 1.         1.00070511 1.00053385]\n",
            "Cluster 9: [0.99918576 0.99394194 0.9914444  0.982482   0.9822798  0.9830367\n",
            " 0.9870209  1.         1.00105545 1.00137007]\n",
            "Cluster 6: [0.98496122 0.98696883 0.98861888 0.99261467 0.99428475 0.99719555\n",
            " 0.99823368 1.         1.00118968 1.00276423]\n",
            "Cluster 2: [0.99853174 0.99939822 1.00008134 1.00074776 1.00121305 1.0009786\n",
            " 1.00106274 1.         1.0011587  1.00301836]\n",
            "Cluster 14: [1.03730084 1.03370249 1.03076122 1.01922596 1.01461318 1.00290849\n",
            " 1.00188293 1.         1.00168542 1.00448336]\n",
            "Cluster 11: [1.01402323 1.01142395 1.01063299 1.00676209 1.00593038 1.00211466\n",
            " 1.00175469 1.         1.00373914 1.01162779]\n",
            "Cluster 7: [1.06799308 1.06865299 1.06728295 1.06447753 1.05853924 1.04292672\n",
            " 1.0326309  1.         1.0091004  1.029579  ]\n",
            "Cluster 3: [1.17562035 1.15987223 1.14720587 1.11529193 1.10191136 1.06821001\n",
            " 1.05093645 1.         1.00901564 1.03152645]\n",
            "\n",
            "Train Cluster Distribution:\n",
            "Cluster 0: 959 samples\n",
            "Cluster 1: 1752 samples\n",
            "Cluster 2: 4785 samples\n",
            "Cluster 3: 48 samples\n",
            "Cluster 4: 164 samples\n",
            "Cluster 5: 1848 samples\n",
            "Cluster 6: 4356 samples\n",
            "Cluster 7: 208 samples\n",
            "Cluster 8: 2584 samples\n",
            "Cluster 9: 1557 samples\n",
            "Cluster 10: 1557 samples\n",
            "Cluster 11: 2124 samples\n",
            "Cluster 12: 895 samples\n",
            "Cluster 13: 169 samples\n",
            "Cluster 14: 1330 samples\n",
            "\n",
            "Validation Cluster Distribution:\n",
            "Cluster 0: 304 samples\n",
            "Cluster 1: 782 samples\n",
            "Cluster 2: 3693 samples\n",
            "Cluster 3: 4 samples\n",
            "Cluster 4: 9 samples\n",
            "Cluster 5: 825 samples\n",
            "Cluster 6: 2601 samples\n",
            "Cluster 7: 41 samples\n",
            "Cluster 8: 1002 samples\n",
            "Cluster 9: 568 samples\n",
            "Cluster 10: 760 samples\n",
            "Cluster 11: 1132 samples\n",
            "Cluster 12: 187 samples\n",
            "Cluster 13: 25 samples\n",
            "Cluster 14: 430 samples\n",
            "\n",
            "Test Cluster Distribution:\n",
            "Cluster 0: 427 samples\n",
            "Cluster 1: 873 samples\n",
            "Cluster 2: 2641 samples\n",
            "Cluster 3: 33 samples\n",
            "Cluster 4: 69 samples\n",
            "Cluster 5: 855 samples\n",
            "Cluster 6: 2077 samples\n",
            "Cluster 7: 63 samples\n",
            "Cluster 8: 1277 samples\n",
            "Cluster 9: 722 samples\n",
            "Cluster 10: 790 samples\n",
            "Cluster 11: 1148 samples\n",
            "Cluster 12: 378 samples\n",
            "Cluster 13: 79 samples\n",
            "Cluster 14: 561 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "def make_model():\n",
        "    \"\"\"\n",
        "    Create a convolutional neural network model with adjusted parameters.\n",
        "    Parameter configuration:\n",
        "    - conv1_filters: 64\n",
        "    - conv2_filters: 128\n",
        "    - dense_units: 256\n",
        "    - dropout_rate: 0.1\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        # First convolutional layer - adjusted to 64 filters\n",
        "        layers.Conv2D(64, (2, 2), activation='relu', input_shape=(4, 5, 1)),\n",
        "        layers.MaxPooling2D((2, 2), padding='same'),\n",
        "\n",
        "        # Second convolutional layer - adjusted to 128 filters\n",
        "        layers.Conv2D(128, (2, 2), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2), padding='same'),\n",
        "\n",
        "        # Flatten layer\n",
        "        layers.Flatten(),\n",
        "\n",
        "        # Fully connected layer - adjusted to 256 units\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.1),  # Adjusted to 10% dropout\n",
        "\n",
        "        # Output layer\n",
        "        layers.Dense(15, activation='softmax')  # Output 15 class probabilities\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Train on the first data set (based on first-order derivatives)\n",
        "print(\"Training on first derivative matrices with adjusted parameters...\")\n",
        "model1 = make_model()\n",
        "history1 = model1.fit(\n",
        "    train_matrices_4x4, train_clusters,\n",
        "    epochs=300,\n",
        "    batch_size=64,  # Adjusted to 64\n",
        "    validation_data=(val_matrices_4x4, val_clusters),\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the first model on the test set\n",
        "test_loss1, test_accuracy1 = model1.evaluate(test_matrices_4x4, test_clusters)\n",
        "print(f\"Adjusted Model - Test Loss: {test_loss1:.4f}, Test Accuracy: {test_accuracy1:.4f}\")\n",
        "\n",
        "# Save model 1\n",
        "model1.save('modelcnn_adjusted.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z5Y_815Hbol",
        "outputId": "88e022ea-9e47-4b95-e048-15b34047842f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on first derivative matrices with adjusted parameters...\n",
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.1852 - loss: 2.4049 - val_accuracy: 0.2987 - val_loss: 2.1547\n",
            "Epoch 2/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.1945 - loss: 2.3365 - val_accuracy: 0.2976 - val_loss: 2.0996\n",
            "Epoch 3/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.2504 - loss: 2.1246 - val_accuracy: 0.4601 - val_loss: 1.6058\n",
            "Epoch 4/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.3948 - loss: 1.6691 - val_accuracy: 0.5267 - val_loss: 1.3940\n",
            "Epoch 5/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4325 - loss: 1.5414 - val_accuracy: 0.5225 - val_loss: 1.3468\n",
            "Epoch 6/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4496 - loss: 1.4689 - val_accuracy: 0.5327 - val_loss: 1.2839\n",
            "Epoch 7/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.4671 - loss: 1.4297 - val_accuracy: 0.5558 - val_loss: 1.2412\n",
            "Epoch 8/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4833 - loss: 1.3800 - val_accuracy: 0.5521 - val_loss: 1.2106\n",
            "Epoch 9/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.4800 - loss: 1.3719 - val_accuracy: 0.5296 - val_loss: 1.2014\n",
            "Epoch 10/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.4666 - loss: 1.3927 - val_accuracy: 0.5973 - val_loss: 1.1233\n",
            "Epoch 11/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5174 - loss: 1.2533 - val_accuracy: 0.6224 - val_loss: 1.0624\n",
            "Epoch 12/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5311 - loss: 1.2152 - val_accuracy: 0.6428 - val_loss: 1.0249\n",
            "Epoch 13/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.5622 - loss: 1.1546 - val_accuracy: 0.5702 - val_loss: 1.1013\n",
            "Epoch 14/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5713 - loss: 1.1173 - val_accuracy: 0.6658 - val_loss: 0.9555\n",
            "Epoch 15/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5839 - loss: 1.0881 - val_accuracy: 0.6554 - val_loss: 0.9101\n",
            "Epoch 16/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.5962 - loss: 1.0428 - val_accuracy: 0.6909 - val_loss: 0.8732\n",
            "Epoch 17/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6174 - loss: 0.9930 - val_accuracy: 0.5623 - val_loss: 1.0877\n",
            "Epoch 18/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6262 - loss: 0.9620 - val_accuracy: 0.4015 - val_loss: 1.4868\n",
            "Epoch 19/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6149 - loss: 0.9700 - val_accuracy: 0.7137 - val_loss: 0.8038\n",
            "Epoch 20/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6383 - loss: 0.9269 - val_accuracy: 0.7151 - val_loss: 0.7865\n",
            "Epoch 21/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6477 - loss: 0.9021 - val_accuracy: 0.6689 - val_loss: 0.8340\n",
            "Epoch 22/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6498 - loss: 0.8940 - val_accuracy: 0.7244 - val_loss: 0.7462\n",
            "Epoch 23/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6695 - loss: 0.8445 - val_accuracy: 0.5296 - val_loss: 1.1208\n",
            "Epoch 24/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.6670 - loss: 0.8490 - val_accuracy: 0.7204 - val_loss: 0.7673\n",
            "Epoch 25/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6856 - loss: 0.7977 - val_accuracy: 0.6921 - val_loss: 0.7894\n",
            "Epoch 26/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6700 - loss: 0.8326 - val_accuracy: 0.7584 - val_loss: 0.6739\n",
            "Epoch 27/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6893 - loss: 0.7801 - val_accuracy: 0.7413 - val_loss: 0.7049\n",
            "Epoch 28/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6935 - loss: 0.7830 - val_accuracy: 0.7446 - val_loss: 0.6873\n",
            "Epoch 29/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6833 - loss: 0.7992 - val_accuracy: 0.7551 - val_loss: 0.6630\n",
            "Epoch 30/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6993 - loss: 0.7643 - val_accuracy: 0.7725 - val_loss: 0.6343\n",
            "Epoch 31/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6988 - loss: 0.7551 - val_accuracy: 0.7102 - val_loss: 0.7146\n",
            "Epoch 32/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6947 - loss: 0.7656 - val_accuracy: 0.5075 - val_loss: 1.2239\n",
            "Epoch 33/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6935 - loss: 0.7771 - val_accuracy: 0.7665 - val_loss: 0.6454\n",
            "Epoch 34/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7035 - loss: 0.7354 - val_accuracy: 0.7213 - val_loss: 0.7250\n",
            "Epoch 35/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.6946 - loss: 0.7625 - val_accuracy: 0.7321 - val_loss: 0.6687\n",
            "Epoch 36/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6936 - loss: 0.7621 - val_accuracy: 0.6672 - val_loss: 0.8081\n",
            "Epoch 37/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6914 - loss: 0.7709 - val_accuracy: 0.7535 - val_loss: 0.6459\n",
            "Epoch 38/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7140 - loss: 0.7195 - val_accuracy: 0.7669 - val_loss: 0.6349\n",
            "Epoch 39/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7146 - loss: 0.7123 - val_accuracy: 0.7471 - val_loss: 0.6451\n",
            "Epoch 40/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7222 - loss: 0.6958 - val_accuracy: 0.7595 - val_loss: 0.6361\n",
            "Epoch 41/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.7239 - loss: 0.6963 - val_accuracy: 0.6600 - val_loss: 0.8079\n",
            "Epoch 42/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7134 - loss: 0.7055 - val_accuracy: 0.7651 - val_loss: 0.6304\n",
            "Epoch 43/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7248 - loss: 0.6863 - val_accuracy: 0.7497 - val_loss: 0.6367\n",
            "Epoch 44/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7193 - loss: 0.7040 - val_accuracy: 0.7091 - val_loss: 0.7068\n",
            "Epoch 45/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.6981 - loss: 0.7528 - val_accuracy: 0.7287 - val_loss: 0.6774\n",
            "Epoch 46/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7101 - loss: 0.7173 - val_accuracy: 0.7136 - val_loss: 0.7168\n",
            "Epoch 47/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7169 - loss: 0.6934 - val_accuracy: 0.4329 - val_loss: 1.4984\n",
            "Epoch 48/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6949 - loss: 0.7460 - val_accuracy: 0.7762 - val_loss: 0.5863\n",
            "Epoch 49/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7190 - loss: 0.6895 - val_accuracy: 0.7827 - val_loss: 0.5858\n",
            "Epoch 50/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7211 - loss: 0.6906 - val_accuracy: 0.7911 - val_loss: 0.5737\n",
            "Epoch 51/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7285 - loss: 0.6764 - val_accuracy: 0.7090 - val_loss: 0.7472\n",
            "Epoch 52/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7088 - loss: 0.7177 - val_accuracy: 0.7715 - val_loss: 0.6023\n",
            "Epoch 53/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7329 - loss: 0.6693 - val_accuracy: 0.6960 - val_loss: 0.7357\n",
            "Epoch 54/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7198 - loss: 0.7009 - val_accuracy: 0.6138 - val_loss: 0.9453\n",
            "Epoch 55/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7139 - loss: 0.6974 - val_accuracy: 0.7561 - val_loss: 0.6133\n",
            "Epoch 56/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7298 - loss: 0.6697 - val_accuracy: 0.7072 - val_loss: 0.7121\n",
            "Epoch 57/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7281 - loss: 0.6650 - val_accuracy: 0.5969 - val_loss: 0.9823\n",
            "Epoch 58/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.7271 - loss: 0.6764 - val_accuracy: 0.7992 - val_loss: 0.5466\n",
            "Epoch 59/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7230 - loss: 0.6861 - val_accuracy: 0.7739 - val_loss: 0.5806\n",
            "Epoch 60/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7374 - loss: 0.6456 - val_accuracy: 0.7106 - val_loss: 0.7113\n",
            "Epoch 61/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7083 - loss: 0.7243 - val_accuracy: 0.7219 - val_loss: 0.6667\n",
            "Epoch 62/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7265 - loss: 0.6672 - val_accuracy: 0.7387 - val_loss: 0.6492\n",
            "Epoch 63/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7191 - loss: 0.6877 - val_accuracy: 0.7874 - val_loss: 0.5738\n",
            "Epoch 64/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7230 - loss: 0.6762 - val_accuracy: 0.7679 - val_loss: 0.5879\n",
            "Epoch 65/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7175 - loss: 0.6941 - val_accuracy: 0.7406 - val_loss: 0.6468\n",
            "Epoch 66/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7243 - loss: 0.6781 - val_accuracy: 0.7990 - val_loss: 0.5485\n",
            "Epoch 67/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7408 - loss: 0.6335 - val_accuracy: 0.7739 - val_loss: 0.5829\n",
            "Epoch 68/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7334 - loss: 0.6569 - val_accuracy: 0.6457 - val_loss: 0.8391\n",
            "Epoch 69/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7134 - loss: 0.6940 - val_accuracy: 0.7560 - val_loss: 0.6093\n",
            "Epoch 70/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7181 - loss: 0.6900 - val_accuracy: 0.7656 - val_loss: 0.6104\n",
            "Epoch 71/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7213 - loss: 0.6844 - val_accuracy: 0.7139 - val_loss: 0.7439\n",
            "Epoch 72/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7275 - loss: 0.6650 - val_accuracy: 0.5739 - val_loss: 1.1336\n",
            "Epoch 73/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7276 - loss: 0.6862 - val_accuracy: 0.7860 - val_loss: 0.5636\n",
            "Epoch 74/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7366 - loss: 0.6487 - val_accuracy: 0.7614 - val_loss: 0.5970\n",
            "Epoch 75/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7203 - loss: 0.6937 - val_accuracy: 0.7819 - val_loss: 0.5840\n",
            "Epoch 76/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7485 - loss: 0.6271 - val_accuracy: 0.5845 - val_loss: 1.0301\n",
            "Epoch 77/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7266 - loss: 0.6758 - val_accuracy: 0.7104 - val_loss: 0.6915\n",
            "Epoch 78/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7323 - loss: 0.6628 - val_accuracy: 0.7861 - val_loss: 0.5520\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7753 - loss: 0.5889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Model - Test Loss: 0.5944, Test Accuracy: 0.7759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XelZfzTFJnvt",
        "outputId": "fe7624d6-9a32-4b4f-c795-90bc8666cc5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.polynomial.polynomial as poly\n",
        "\n",
        "# Set file path\n",
        "zip_file_path = '/content/drive/My Drive/SP500_data2020-2024.zip'\n",
        "output_dir = '/content/SP500_data2020-2024'\n",
        "\n",
        "# Extract the zip file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(\"Files extracted to:\", output_dir)\n",
        "csv_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.csv')])\n",
        "print(f\"Found {len(csv_files)} CSV files.\")\n",
        "print(\"Sample files:\", csv_files[:5])\n",
        "\n",
        "# Read all CSV files into a dictionary\n",
        "sp500_data = {}\n",
        "for csv_file in csv_files:\n",
        "    symbol = csv_file.replace('.csv', '')\n",
        "    file_path = os.path.join(output_dir, csv_file)\n",
        "    sp500_data[symbol] = pd.read_csv(file_path)\n",
        "\n",
        "# Filter out symbols with the most common data length\n",
        "def filter_symbols_by_most_common_length(sp500_data):\n",
        "    lengths = {symbol: len(data) for symbol, data in sp500_data.items()}\n",
        "    length_counts = pd.Series(lengths).value_counts()\n",
        "    most_common_length = length_counts.idxmax()\n",
        "    print(f\"Most common length: {most_common_length}, Count: {length_counts.max()}\")\n",
        "    filtered_symbols = [symbol for symbol, length in lengths.items() if length == most_common_length]\n",
        "    filtered_data = {symbol: sp500_data[symbol] for symbol in filtered_symbols}\n",
        "    return filtered_data, most_common_length\n",
        "\n",
        "filtered_sp500_data, most_common_length = filter_symbols_by_most_common_length(sp500_data)\n",
        "print(f\"Filtered data count: {len(filtered_sp500_data)}\")\n",
        "\n",
        "# Parameters\n",
        "window_size = 11\n",
        "\n",
        "# Output data\n",
        "matrices_4x4 = []\n",
        "labels3 = []\n",
        "\n",
        "# Generate matrices and labels\n",
        "for symbol, data in filtered_sp500_data.items():\n",
        "    data['Open'] = pd.to_numeric(data['Open'], errors='coerce')\n",
        "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
        "    open_values = data['Open'].values[2:]\n",
        "    close_values = data['Close'].values[2:]\n",
        "\n",
        "    num_samples = len(data) - window_size + 1\n",
        "    for start in range(num_samples - 2):\n",
        "        open_row = open_values[start:start + window_size]\n",
        "        close_row = close_values[start:start + window_size]\n",
        "        normalization_factor = close_row[-2]\n",
        "        open_row = open_row / normalization_factor\n",
        "        close_row = close_row / normalization_factor\n",
        "\n",
        "        combined = np.array([open_row[i // 2] if i % 2 == 0 else close_row[i // 2] for i in range(window_size * 2)])\n",
        "        matrix_4x4 = combined[:-2].reshape(4, 5, 1)\n",
        "        label3 = 1 if close_row[-1] > close_row[-2] else 0\n",
        "\n",
        "        matrices_4x4.append(matrix_4x4)\n",
        "        labels3.append(label3)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "matrices_4x4 = np.array(matrices_4x4)\n",
        "labels3 = np.array(labels3)\n",
        "\n",
        "# Output shapes\n",
        "print(f\"4x4 matrices shape: {matrices_4x4.shape}\")\n",
        "print(f\"Labels3 shape: {labels3.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4agPPJ1-HrU2",
        "outputId": "5ff8216d-7a31-4a86-d98d-815d53f1baff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/SP500_data2020-2024\n",
            "Found 501 CSV files.\n",
            "Sample files: ['A.csv', 'AAPL.csv', 'ABBV.csv', 'ABNB.csv', 'ABT.csv']\n",
            "Most common length: 1239, Count: 489\n",
            "Filtered data count: 489\n",
            "4x4 matrices shape: (600003, 4, 5, 1)\n",
            "Labels3 shape: (600003,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictionscnn_pro = model1.predict(matrices_4x4)\n",
        "\n",
        "# predict\n",
        "predicted_classes = np.argmax(predictionscnn_pro, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBpjGuuaJt-P",
        "outputId": "cbf6b48e-ae89-4cfe-8851-c28d44ab877f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18751/18751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get the total number of classes\n",
        "num_classes = predictionscnn_pro.shape[1]\n",
        "\n",
        "# Initialize the final prediction array\n",
        "final_predictions = np.zeros(predictionscnn_pro.shape[0])  # Initialize to 0\n",
        "\n",
        "# Define which class(es) should be marked as 1\n",
        "target_classes = {sorted_indices[-1]}  # Assume sorted_indices is predefined\n",
        "\n",
        "# Iterate over each sample\n",
        "for i in range(predictionscnn_pro.shape[0]):\n",
        "    # Get the predicted class for the current sample\n",
        "    class_label = predicted_classes[i]\n",
        "\n",
        "    # If the class is in the target set, label as 1; otherwise, 0\n",
        "    final_predictions[i] = 1 if class_label in target_classes else 0\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(final_predictions == labels3)\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(labels3, final_predictions, labels=[0, 1])\n",
        "\n",
        "# Compute Precision\n",
        "tp = cm[1, 1]  # True Positives\n",
        "fp = cm[0, 1]  # False Positives\n",
        "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "\n",
        "# Compute Recall\n",
        "fn = cm[1, 0]  # False Negatives\n",
        "recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "\n",
        "# Compute F1 Score\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(\"Final Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Precision (TP / (TP + FP)): {precision:.2%}\")\n",
        "print(f\"Recall (TP / (TP + FN)): {recall:.2%}\")\n",
        "print(f\"F1 Score: {f1:.2%}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fQhgdzpKG_i",
        "outputId": "85869b3b-a1a8-4331-d19d-6c66c2fc4a9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Results:\n",
            "Accuracy: 48.39%\n",
            "Precision (TP / (TP + FP)): 61.54%\n",
            "Recall (TP / (TP + FN)): 0.82%\n",
            "F1 Score: 1.62%\n",
            "Confusion Matrix:\n",
            "[[287794   1597]\n",
            " [308057   2555]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def make_model():\n",
        "    \"\"\"\n",
        "    Create an MLP model using the best parameter configuration:\n",
        "    - hidden_units1: 128\n",
        "    - hidden_units2: 128\n",
        "    - dropout_rate: 0.0\n",
        "    - learning_rate: 0.001\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(4, 5, 1)),  # Flatten input from (4, 5, 1) to (20,)\n",
        "        tf.keras.layers.Dense(units=128, activation='relu'),  # First hidden layer with 128 units\n",
        "        tf.keras.layers.Dense(units=128, activation='relu'),  # Second hidden layer with 128 units\n",
        "        # Note: dropout_rate = 0.0 means no Dropout layer used\n",
        "        tf.keras.layers.Dense(units=15, activation='softmax')  # Output layer with 15 classes\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create and train the model\n",
        "model1 = make_model()\n",
        "history1 = model1.fit(\n",
        "    train_matrices_4x4, train_clusters,\n",
        "    epochs=300,\n",
        "    batch_size=64,  # Using best batch_size = 64\n",
        "    validation_data=(val_matrices_4x4, val_clusters),\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss1, test_accuracy1 = model1.evaluate(test_matrices_4x4, test_clusters)\n",
        "print(f\"Optimized MLP Model - Test Loss: {test_loss1:.4f}, Test Accuracy: {test_accuracy1:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "model1.save('optimized_mlp_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toakzaWnKh4m",
        "outputId": "f731037c-0dc8-4494-a6b3-b288f23858bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.1844 - loss: 2.3891 - val_accuracy: 0.2104 - val_loss: 2.1767\n",
            "Epoch 2/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.1941 - loss: 2.3370 - val_accuracy: 0.2104 - val_loss: 2.1225\n",
            "Epoch 3/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2038 - loss: 2.2914 - val_accuracy: 0.3092 - val_loss: 2.0367\n",
            "Epoch 4/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.2378 - loss: 2.1770 - val_accuracy: 0.3339 - val_loss: 1.9884\n",
            "Epoch 5/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.2607 - loss: 2.0850 - val_accuracy: 0.3902 - val_loss: 1.8509\n",
            "Epoch 6/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.2916 - loss: 1.9893 - val_accuracy: 0.4303 - val_loss: 1.8005\n",
            "Epoch 7/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.3185 - loss: 1.8841 - val_accuracy: 0.4248 - val_loss: 1.6927\n",
            "Epoch 8/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3561 - loss: 1.7875 - val_accuracy: 0.4015 - val_loss: 1.6828\n",
            "Epoch 9/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.3828 - loss: 1.6992 - val_accuracy: 0.5013 - val_loss: 1.5037\n",
            "Epoch 10/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4093 - loss: 1.6271 - val_accuracy: 0.4294 - val_loss: 1.6358\n",
            "Epoch 11/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4320 - loss: 1.5633 - val_accuracy: 0.5292 - val_loss: 1.3936\n",
            "Epoch 12/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.4450 - loss: 1.5101 - val_accuracy: 0.5279 - val_loss: 1.3628\n",
            "Epoch 13/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.4395 - loss: 1.4955 - val_accuracy: 0.4932 - val_loss: 1.3790\n",
            "Epoch 14/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4718 - loss: 1.4308 - val_accuracy: 0.4742 - val_loss: 1.4178\n",
            "Epoch 15/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.4768 - loss: 1.4041 - val_accuracy: 0.5792 - val_loss: 1.2284\n",
            "Epoch 16/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5071 - loss: 1.3427 - val_accuracy: 0.5524 - val_loss: 1.2799\n",
            "Epoch 17/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5133 - loss: 1.3067 - val_accuracy: 0.5583 - val_loss: 1.2281\n",
            "Epoch 18/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5160 - loss: 1.2878 - val_accuracy: 0.6054 - val_loss: 1.1705\n",
            "Epoch 19/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5415 - loss: 1.2383 - val_accuracy: 0.5673 - val_loss: 1.1903\n",
            "Epoch 20/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5329 - loss: 1.2427 - val_accuracy: 0.6463 - val_loss: 1.0976\n",
            "Epoch 21/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5623 - loss: 1.1847 - val_accuracy: 0.5961 - val_loss: 1.1609\n",
            "Epoch 22/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5698 - loss: 1.1617 - val_accuracy: 0.5740 - val_loss: 1.1212\n",
            "Epoch 23/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5749 - loss: 1.1387 - val_accuracy: 0.6656 - val_loss: 1.0297\n",
            "Epoch 24/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.5909 - loss: 1.1122 - val_accuracy: 0.6401 - val_loss: 1.0435\n",
            "Epoch 25/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6033 - loss: 1.0845 - val_accuracy: 0.6397 - val_loss: 1.0307\n",
            "Epoch 26/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6080 - loss: 1.0599 - val_accuracy: 0.6940 - val_loss: 0.9562\n",
            "Epoch 27/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6199 - loss: 1.0380 - val_accuracy: 0.6752 - val_loss: 0.9718\n",
            "Epoch 28/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.6207 - loss: 1.0229 - val_accuracy: 0.6324 - val_loss: 1.0456\n",
            "Epoch 29/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.6217 - loss: 1.0194 - val_accuracy: 0.5405 - val_loss: 1.1608\n",
            "Epoch 30/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6206 - loss: 1.0096 - val_accuracy: 0.7176 - val_loss: 0.8843\n",
            "Epoch 31/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6566 - loss: 0.9514 - val_accuracy: 0.7128 - val_loss: 0.8756\n",
            "Epoch 32/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6439 - loss: 0.9519 - val_accuracy: 0.5423 - val_loss: 1.1289\n",
            "Epoch 33/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6491 - loss: 0.9458 - val_accuracy: 0.5221 - val_loss: 1.1429\n",
            "Epoch 34/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6368 - loss: 0.9605 - val_accuracy: 0.7040 - val_loss: 0.8697\n",
            "Epoch 35/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6526 - loss: 0.9261 - val_accuracy: 0.5816 - val_loss: 1.0412\n",
            "Epoch 36/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6649 - loss: 0.8987 - val_accuracy: 0.7364 - val_loss: 0.8295\n",
            "Epoch 37/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6577 - loss: 0.9051 - val_accuracy: 0.6619 - val_loss: 0.9238\n",
            "Epoch 38/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6827 - loss: 0.8594 - val_accuracy: 0.4307 - val_loss: 1.3043\n",
            "Epoch 39/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6543 - loss: 0.8964 - val_accuracy: 0.6532 - val_loss: 0.8998\n",
            "Epoch 40/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 0.8478 - val_accuracy: 0.5331 - val_loss: 1.1577\n",
            "Epoch 41/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6800 - loss: 0.8485 - val_accuracy: 0.7246 - val_loss: 0.8070\n",
            "Epoch 42/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6800 - loss: 0.8448 - val_accuracy: 0.7223 - val_loss: 0.7931\n",
            "Epoch 43/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6677 - loss: 0.8613 - val_accuracy: 0.6540 - val_loss: 0.8630\n",
            "Epoch 44/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6933 - loss: 0.8103 - val_accuracy: 0.5404 - val_loss: 1.0745\n",
            "Epoch 45/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6607 - loss: 0.8723 - val_accuracy: 0.6934 - val_loss: 0.8068\n",
            "Epoch 46/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7005 - loss: 0.7874 - val_accuracy: 0.6329 - val_loss: 0.8959\n",
            "Epoch 47/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6825 - loss: 0.8122 - val_accuracy: 0.7069 - val_loss: 0.8022\n",
            "Epoch 48/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6999 - loss: 0.7897 - val_accuracy: 0.7716 - val_loss: 0.7110\n",
            "Epoch 49/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6962 - loss: 0.7910 - val_accuracy: 0.7162 - val_loss: 0.7912\n",
            "Epoch 50/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7212 - loss: 0.7521 - val_accuracy: 0.6676 - val_loss: 0.8247\n",
            "Epoch 51/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7057 - loss: 0.7716 - val_accuracy: 0.6530 - val_loss: 0.8392\n",
            "Epoch 52/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7134 - loss: 0.7583 - val_accuracy: 0.7618 - val_loss: 0.7052\n",
            "Epoch 53/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7083 - loss: 0.7598 - val_accuracy: 0.7500 - val_loss: 0.7183\n",
            "Epoch 54/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6947 - loss: 0.7735 - val_accuracy: 0.5038 - val_loss: 1.1572\n",
            "Epoch 55/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.7666 - val_accuracy: 0.7503 - val_loss: 0.7194\n",
            "Epoch 56/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7126 - loss: 0.7444 - val_accuracy: 0.7757 - val_loss: 0.6850\n",
            "Epoch 57/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7251 - loss: 0.7295 - val_accuracy: 0.7018 - val_loss: 0.7889\n",
            "Epoch 58/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7192 - loss: 0.7228 - val_accuracy: 0.5309 - val_loss: 1.0884\n",
            "Epoch 59/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6964 - loss: 0.7693 - val_accuracy: 0.6824 - val_loss: 0.8054\n",
            "Epoch 60/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7036 - loss: 0.7543 - val_accuracy: 0.7784 - val_loss: 0.6754\n",
            "Epoch 61/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7167 - loss: 0.7278 - val_accuracy: 0.4973 - val_loss: 1.2194\n",
            "Epoch 62/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7047 - loss: 0.7435 - val_accuracy: 0.6396 - val_loss: 0.8644\n",
            "Epoch 63/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7036 - loss: 0.7532 - val_accuracy: 0.5352 - val_loss: 1.1430\n",
            "Epoch 64/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7010 - loss: 0.7527 - val_accuracy: 0.5923 - val_loss: 0.9774\n",
            "Epoch 65/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6984 - loss: 0.7629 - val_accuracy: 0.7711 - val_loss: 0.6593\n",
            "Epoch 66/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7258 - loss: 0.7097 - val_accuracy: 0.7429 - val_loss: 0.6841\n",
            "Epoch 67/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7217 - loss: 0.7056 - val_accuracy: 0.7548 - val_loss: 0.6852\n",
            "Epoch 68/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7252 - loss: 0.7114 - val_accuracy: 0.7525 - val_loss: 0.6649\n",
            "Epoch 69/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7358 - loss: 0.6793 - val_accuracy: 0.6947 - val_loss: 0.7491\n",
            "Epoch 70/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.7102 - val_accuracy: 0.7603 - val_loss: 0.6616\n",
            "Epoch 71/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7052 - loss: 0.7351 - val_accuracy: 0.7434 - val_loss: 0.6785\n",
            "Epoch 72/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7127 - loss: 0.7323 - val_accuracy: 0.7465 - val_loss: 0.6675\n",
            "Epoch 73/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7145 - loss: 0.7237 - val_accuracy: 0.5416 - val_loss: 1.1200\n",
            "Epoch 74/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7047 - loss: 0.7460 - val_accuracy: 0.7653 - val_loss: 0.6459\n",
            "Epoch 75/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6920 - loss: 0.7601 - val_accuracy: 0.7886 - val_loss: 0.6204\n",
            "Epoch 76/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7366 - loss: 0.6766 - val_accuracy: 0.7500 - val_loss: 0.6670\n",
            "Epoch 77/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7099 - loss: 0.7238 - val_accuracy: 0.7722 - val_loss: 0.6401\n",
            "Epoch 78/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7367 - loss: 0.6787 - val_accuracy: 0.5882 - val_loss: 1.0106\n",
            "Epoch 79/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7185 - loss: 0.7088 - val_accuracy: 0.7306 - val_loss: 0.6918\n",
            "Epoch 80/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7275 - loss: 0.6893 - val_accuracy: 0.4954 - val_loss: 1.2996\n",
            "Epoch 81/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7267 - loss: 0.7043 - val_accuracy: 0.7120 - val_loss: 0.7241\n",
            "Epoch 82/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.6834 - val_accuracy: 0.7186 - val_loss: 0.7030\n",
            "Epoch 83/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.7310 - loss: 0.6869 - val_accuracy: 0.7857 - val_loss: 0.6136\n",
            "Epoch 84/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7274 - loss: 0.6833 - val_accuracy: 0.7439 - val_loss: 0.6627\n",
            "Epoch 85/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 0.6596 - val_accuracy: 0.7594 - val_loss: 0.6379\n",
            "Epoch 86/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7285 - loss: 0.6831 - val_accuracy: 0.5477 - val_loss: 1.0942\n",
            "Epoch 87/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7177 - loss: 0.7095 - val_accuracy: 0.7491 - val_loss: 0.6727\n",
            "Epoch 88/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7420 - loss: 0.6538 - val_accuracy: 0.5089 - val_loss: 1.2407\n",
            "Epoch 89/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6954 - loss: 0.8060 - val_accuracy: 0.6295 - val_loss: 0.9086\n",
            "Epoch 90/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6964 - loss: 0.7884 - val_accuracy: 0.6820 - val_loss: 0.7800\n",
            "Epoch 91/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7411 - loss: 0.6543 - val_accuracy: 0.8009 - val_loss: 0.5897\n",
            "Epoch 92/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7296 - loss: 0.6667 - val_accuracy: 0.7941 - val_loss: 0.5947\n",
            "Epoch 93/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7430 - loss: 0.6535 - val_accuracy: 0.7260 - val_loss: 0.6870\n",
            "Epoch 94/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7578 - loss: 0.6208 - val_accuracy: 0.7726 - val_loss: 0.6120\n",
            "Epoch 95/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7161 - loss: 0.6980 - val_accuracy: 0.7397 - val_loss: 0.6723\n",
            "Epoch 96/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7421 - loss: 0.6495 - val_accuracy: 0.6184 - val_loss: 0.9038\n",
            "Epoch 97/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7185 - loss: 0.6951 - val_accuracy: 0.7528 - val_loss: 0.6426\n",
            "Epoch 98/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7416 - loss: 0.6470 - val_accuracy: 0.7537 - val_loss: 0.6390\n",
            "Epoch 99/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7400 - loss: 0.6472 - val_accuracy: 0.7417 - val_loss: 0.6392\n",
            "Epoch 100/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7327 - loss: 0.6662 - val_accuracy: 0.7814 - val_loss: 0.6079\n",
            "Epoch 101/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7444 - loss: 0.6461 - val_accuracy: 0.6777 - val_loss: 0.7645\n",
            "Epoch 102/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7235 - loss: 0.6770 - val_accuracy: 0.7705 - val_loss: 0.6347\n",
            "Epoch 103/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7242 - loss: 0.6892 - val_accuracy: 0.4926 - val_loss: 1.5013\n",
            "Epoch 104/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7152 - loss: 0.7121 - val_accuracy: 0.8027 - val_loss: 0.5809\n",
            "Epoch 105/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7325 - loss: 0.6624 - val_accuracy: 0.7796 - val_loss: 0.6007\n",
            "Epoch 106/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7315 - loss: 0.6712 - val_accuracy: 0.7567 - val_loss: 0.6306\n",
            "Epoch 107/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7468 - loss: 0.6427 - val_accuracy: 0.7892 - val_loss: 0.5830\n",
            "Epoch 108/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7300 - loss: 0.6696 - val_accuracy: 0.7574 - val_loss: 0.6649\n",
            "Epoch 109/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.7078 - loss: 0.7411 - val_accuracy: 0.6868 - val_loss: 0.7316\n",
            "Epoch 110/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7296 - loss: 0.6715 - val_accuracy: 0.7870 - val_loss: 0.5873\n",
            "Epoch 111/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7497 - loss: 0.6220 - val_accuracy: 0.7076 - val_loss: 0.7755\n",
            "Epoch 112/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7315 - loss: 0.6722 - val_accuracy: 0.8036 - val_loss: 0.5596\n",
            "Epoch 113/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7472 - loss: 0.6343 - val_accuracy: 0.6979 - val_loss: 0.7243\n",
            "Epoch 114/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7323 - loss: 0.6634 - val_accuracy: 0.6061 - val_loss: 0.9765\n",
            "Epoch 115/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7136 - loss: 0.6982 - val_accuracy: 0.6769 - val_loss: 0.7570\n",
            "Epoch 116/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7271 - loss: 0.6735 - val_accuracy: 0.8118 - val_loss: 0.5572\n",
            "Epoch 117/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.6283 - val_accuracy: 0.8004 - val_loss: 0.5611\n",
            "Epoch 118/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7486 - loss: 0.6193 - val_accuracy: 0.8035 - val_loss: 0.5657\n",
            "Epoch 119/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7379 - loss: 0.6516 - val_accuracy: 0.7872 - val_loss: 0.5893\n",
            "Epoch 120/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7403 - loss: 0.6524 - val_accuracy: 0.8136 - val_loss: 0.5549\n",
            "Epoch 121/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7567 - loss: 0.6083 - val_accuracy: 0.7232 - val_loss: 0.6748\n",
            "Epoch 122/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.6425 - val_accuracy: 0.5050 - val_loss: 1.2730\n",
            "Epoch 123/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6976 - loss: 0.7702 - val_accuracy: 0.6984 - val_loss: 0.7330\n",
            "Epoch 124/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7424 - loss: 0.6396 - val_accuracy: 0.7943 - val_loss: 0.5690\n",
            "Epoch 125/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7627 - loss: 0.5994 - val_accuracy: 0.7500 - val_loss: 0.6497\n",
            "Epoch 126/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7348 - loss: 0.6631 - val_accuracy: 0.7597 - val_loss: 0.6282\n",
            "Epoch 127/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7365 - loss: 0.6534 - val_accuracy: 0.7756 - val_loss: 0.5879\n",
            "Epoch 128/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7581 - loss: 0.6146 - val_accuracy: 0.7330 - val_loss: 0.6531\n",
            "Epoch 129/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7357 - loss: 0.6526 - val_accuracy: 0.8064 - val_loss: 0.5542\n",
            "Epoch 130/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7436 - loss: 0.6307 - val_accuracy: 0.7442 - val_loss: 0.6336\n",
            "Epoch 131/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7427 - loss: 0.6317 - val_accuracy: 0.7962 - val_loss: 0.5664\n",
            "Epoch 132/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7540 - loss: 0.6184 - val_accuracy: 0.7298 - val_loss: 0.6668\n",
            "Epoch 133/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7267 - loss: 0.6756 - val_accuracy: 0.7946 - val_loss: 0.5677\n",
            "Epoch 134/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7195 - loss: 0.6833 - val_accuracy: 0.8153 - val_loss: 0.5489\n",
            "Epoch 135/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7519 - loss: 0.6163 - val_accuracy: 0.8083 - val_loss: 0.5466\n",
            "Epoch 136/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7326 - loss: 0.6655 - val_accuracy: 0.7906 - val_loss: 0.5655\n",
            "Epoch 137/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7506 - loss: 0.6217 - val_accuracy: 0.7789 - val_loss: 0.5920\n",
            "Epoch 138/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7474 - loss: 0.6298 - val_accuracy: 0.7302 - val_loss: 0.6587\n",
            "Epoch 139/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7449 - loss: 0.6330 - val_accuracy: 0.7974 - val_loss: 0.5562\n",
            "Epoch 140/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7441 - loss: 0.6392 - val_accuracy: 0.5862 - val_loss: 1.0404\n",
            "Epoch 141/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7409 - loss: 0.6496 - val_accuracy: 0.8118 - val_loss: 0.5395\n",
            "Epoch 142/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7459 - loss: 0.6316 - val_accuracy: 0.7249 - val_loss: 0.6591\n",
            "Epoch 143/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7498 - loss: 0.6268 - val_accuracy: 0.6754 - val_loss: 0.7809\n",
            "Epoch 144/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7468 - loss: 0.6254 - val_accuracy: 0.6201 - val_loss: 0.9155\n",
            "Epoch 145/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7339 - loss: 0.6505 - val_accuracy: 0.7981 - val_loss: 0.5613\n",
            "Epoch 146/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7392 - loss: 0.6560 - val_accuracy: 0.8005 - val_loss: 0.5455\n",
            "Epoch 147/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7643 - loss: 0.5901 - val_accuracy: 0.7759 - val_loss: 0.5703\n",
            "Epoch 148/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7241 - loss: 0.6769 - val_accuracy: 0.7821 - val_loss: 0.5680\n",
            "Epoch 149/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7488 - loss: 0.6200 - val_accuracy: 0.8114 - val_loss: 0.5360\n",
            "Epoch 150/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7354 - loss: 0.6590 - val_accuracy: 0.7480 - val_loss: 0.6361\n",
            "Epoch 151/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7322 - loss: 0.6687 - val_accuracy: 0.7759 - val_loss: 0.5847\n",
            "Epoch 152/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7689 - loss: 0.5773 - val_accuracy: 0.6603 - val_loss: 0.8212\n",
            "Epoch 153/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7325 - loss: 0.6563 - val_accuracy: 0.7328 - val_loss: 0.6436\n",
            "Epoch 154/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7272 - loss: 0.6836 - val_accuracy: 0.7806 - val_loss: 0.5654\n",
            "Epoch 155/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7453 - loss: 0.6318 - val_accuracy: 0.7790 - val_loss: 0.5761\n",
            "Epoch 156/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7446 - loss: 0.6353 - val_accuracy: 0.7379 - val_loss: 0.6595\n",
            "Epoch 157/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7531 - loss: 0.6105 - val_accuracy: 0.7914 - val_loss: 0.5597\n",
            "Epoch 158/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7503 - loss: 0.6084 - val_accuracy: 0.7065 - val_loss: 0.7041\n",
            "Epoch 159/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7412 - loss: 0.6411 - val_accuracy: 0.7708 - val_loss: 0.5966\n",
            "Epoch 160/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.5994 - val_accuracy: 0.8022 - val_loss: 0.5476\n",
            "Epoch 161/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7124 - loss: 0.6871 - val_accuracy: 0.7812 - val_loss: 0.5906\n",
            "Epoch 162/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7381 - loss: 0.6289 - val_accuracy: 0.5719 - val_loss: 1.0962\n",
            "Epoch 163/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7206 - loss: 0.6856 - val_accuracy: 0.7494 - val_loss: 0.6218\n",
            "Epoch 164/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7605 - loss: 0.5927 - val_accuracy: 0.6549 - val_loss: 0.8126\n",
            "Epoch 165/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7489 - loss: 0.6198 - val_accuracy: 0.7425 - val_loss: 0.6264\n",
            "Epoch 166/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7497 - loss: 0.6238 - val_accuracy: 0.7517 - val_loss: 0.6149\n",
            "Epoch 167/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.7316 - loss: 0.6563 - val_accuracy: 0.8049 - val_loss: 0.5476\n",
            "Epoch 168/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7485 - loss: 0.6238 - val_accuracy: 0.7495 - val_loss: 0.6503\n",
            "Epoch 169/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7443 - loss: 0.6222 - val_accuracy: 0.7103 - val_loss: 0.6855\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7890 - loss: 0.5562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized MLP Model - Test Loss: 0.5611, Test Accuracy: 0.7886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model prediction probabilities\n",
        "predictionscnn_pro = model1.predict(matrices_4x4)\n",
        "\n",
        "# Get predicted class labels\n",
        "predicted_classes = np.argmax(predictionscnn_pro, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC20hfQjKl6m",
        "outputId": "e2dc902c-30a0-4281-c9ae-cadd65a451b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18751/18751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get the total number of classes\n",
        "num_classes = predictionscnn_pro.shape[1]\n",
        "\n",
        "# Initialize the final predictions array\n",
        "final_predictions = np.zeros(predictionscnn_pro.shape[0])  # Initialize with 0\n",
        "\n",
        "# Define the target class(es) to be labeled as 1\n",
        "target_classes = {sorted_indices[-1]}  # Assume sorted_indices is predefined\n",
        "\n",
        "# Iterate through each sample\n",
        "for i in range(predictionscnn_pro.shape[0]):\n",
        "    # Get the predicted class label for the current sample\n",
        "    class_label = predicted_classes[i]\n",
        "\n",
        "    # If the class is in the target set, assign 1; otherwise, 0\n",
        "    final_predictions[i] = 1 if class_label in target_classes else 0\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(final_predictions == labels3)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(labels3, final_predictions, labels=[0, 1])\n",
        "\n",
        "# Calculate Precision\n",
        "tp = cm[1, 1]  # True Positives\n",
        "fp = cm[0, 1]  # False Positives\n",
        "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "\n",
        "# Calculate Recall\n",
        "fn = cm[1, 0]  # False Negatives\n",
        "recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(\"Final Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Precision (TP / (TP + FP)): {precision:.2%}\")\n",
        "print(f\"Recall (TP / (TP + FN)): {recall:.2%}\")\n",
        "print(f\"F1 Score: {f1:.2%}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIcfTDnKK5qT",
        "outputId": "eb83b893-e31c-4e1d-8090-d199652fc094"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Results:\n",
            "Accuracy: 48.42%\n",
            "Precision (TP / (TP + FP)): 59.40%\n",
            "Recall (TP / (TP + FN)): 1.13%\n",
            "F1 Score: 2.22%\n",
            "Confusion Matrix:\n",
            "[[286991   2400]\n",
            " [307100   3512]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "def build_lstm_model():\n",
        "    \"\"\"\n",
        "    Build an LSTM model with adjusted parameters.\n",
        "    Configuration:\n",
        "    - input_shape: (20, 1)  # 20 time steps, 1 feature per step\n",
        "    - lstm_units: 64\n",
        "    - num_layers: 2 (two LSTM layers)\n",
        "    - dense_units: 15\n",
        "    - dropout_rate: 0.0\n",
        "    - learning_rate: 0.0005\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential([\n",
        "        keras.layers.Input(shape=(20, 1)),  # Set input to 20 time steps\n",
        "\n",
        "        # First LSTM layer - 64 units, return sequences for next layer\n",
        "        keras.layers.LSTM(64, return_sequences=True),\n",
        "\n",
        "        # Second LSTM layer - 64 units, do not return sequences\n",
        "        keras.layers.LSTM(64, return_sequences=False),\n",
        "\n",
        "        # Output layer - 15 units\n",
        "        keras.layers.Dense(15, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Use Adam optimizer with adjusted learning rate of 0.0005\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "lstm_model = build_lstm_model()\n",
        "\n",
        "# Reshape training data to (samples, 20, 1)\n",
        "train_data = train_matrices_4x4.reshape(-1, 20, 1)\n",
        "val_data = val_matrices_4x4.reshape(-1, 20, 1)\n",
        "test_data = test_matrices_4x4.reshape(-1, 20, 1)\n",
        "\n",
        "# Train the model\n",
        "history_lstm = lstm_model.fit(\n",
        "    train_data, train_clusters,\n",
        "    epochs=300,\n",
        "    batch_size=64,\n",
        "    validation_data=(val_data, val_clusters),\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss_lstm, test_accuracy_lstm = lstm_model.evaluate(\n",
        "    test_data, test_clusters\n",
        ")\n",
        "\n",
        "print(f\"Optimized LSTM Model (20 timesteps) - Test Loss: {test_loss_lstm:.4f}, Test Accuracy: {test_accuracy_lstm:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "lstm_model.save('optimized_lstm_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4ibvaK1KuZA",
        "outputId": "956ed521-8406-4bf6-ef1f-0e321b3fdf5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.1921 - loss: 2.4044 - val_accuracy: 0.2104 - val_loss: 2.1821\n",
            "Epoch 2/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.1931 - loss: 2.3469 - val_accuracy: 0.3048 - val_loss: 2.1593\n",
            "Epoch 3/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.2183 - loss: 2.2520 - val_accuracy: 0.3955 - val_loss: 1.8175\n",
            "Epoch 4/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.2762 - loss: 2.0068 - val_accuracy: 0.3475 - val_loss: 1.8234\n",
            "Epoch 5/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 43ms/step - accuracy: 0.3066 - loss: 1.9012 - val_accuracy: 0.4683 - val_loss: 1.5626\n",
            "Epoch 6/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.3599 - loss: 1.7434 - val_accuracy: 0.4226 - val_loss: 1.5926\n",
            "Epoch 7/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.3709 - loss: 1.7141 - val_accuracy: 0.4761 - val_loss: 1.4526\n",
            "Epoch 8/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 39ms/step - accuracy: 0.4138 - loss: 1.5624 - val_accuracy: 0.5652 - val_loss: 1.2813\n",
            "Epoch 9/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.4390 - loss: 1.4825 - val_accuracy: 0.5877 - val_loss: 1.2171\n",
            "Epoch 10/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.4661 - loss: 1.4102 - val_accuracy: 0.5662 - val_loss: 1.2144\n",
            "Epoch 11/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.4749 - loss: 1.3746 - val_accuracy: 0.6185 - val_loss: 1.0974\n",
            "Epoch 12/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.5230 - loss: 1.2587 - val_accuracy: 0.5227 - val_loss: 1.2089\n",
            "Epoch 13/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.5187 - loss: 1.2544 - val_accuracy: 0.4873 - val_loss: 1.3119\n",
            "Epoch 14/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.5563 - loss: 1.1756 - val_accuracy: 0.4891 - val_loss: 1.2673\n",
            "Epoch 15/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.5592 - loss: 1.1417 - val_accuracy: 0.6919 - val_loss: 0.9105\n",
            "Epoch 16/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 49ms/step - accuracy: 0.5840 - loss: 1.0867 - val_accuracy: 0.6883 - val_loss: 0.9042\n",
            "Epoch 17/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.5921 - loss: 1.0574 - val_accuracy: 0.5967 - val_loss: 1.0200\n",
            "Epoch 18/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.5779 - loss: 1.0915 - val_accuracy: 0.6986 - val_loss: 0.8738\n",
            "Epoch 19/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.5949 - loss: 1.0393 - val_accuracy: 0.7072 - val_loss: 0.8606\n",
            "Epoch 20/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6260 - loss: 0.9700 - val_accuracy: 0.5929 - val_loss: 1.0057\n",
            "Epoch 21/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6087 - loss: 0.9936 - val_accuracy: 0.6253 - val_loss: 0.9610\n",
            "Epoch 22/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6249 - loss: 0.9576 - val_accuracy: 0.6555 - val_loss: 0.8847\n",
            "Epoch 23/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.6395 - loss: 0.9248 - val_accuracy: 0.6886 - val_loss: 0.8513\n",
            "Epoch 24/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.6443 - loss: 0.9089 - val_accuracy: 0.6827 - val_loss: 0.8201\n",
            "Epoch 25/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.6314 - loss: 0.9301 - val_accuracy: 0.5572 - val_loss: 1.0928\n",
            "Epoch 26/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.6389 - loss: 0.9249 - val_accuracy: 0.6485 - val_loss: 0.8767\n",
            "Epoch 27/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6560 - loss: 0.8694 - val_accuracy: 0.7325 - val_loss: 0.7401\n",
            "Epoch 28/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.6672 - loss: 0.8442 - val_accuracy: 0.7481 - val_loss: 0.7259\n",
            "Epoch 29/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6662 - loss: 0.8410 - val_accuracy: 0.6428 - val_loss: 0.8924\n",
            "Epoch 30/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.6655 - loss: 0.8446 - val_accuracy: 0.5907 - val_loss: 1.0359\n",
            "Epoch 31/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.6879 - loss: 0.7956 - val_accuracy: 0.6252 - val_loss: 0.9240\n",
            "Epoch 32/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.6761 - loss: 0.8137 - val_accuracy: 0.6506 - val_loss: 0.8562\n",
            "Epoch 33/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.6860 - loss: 0.7973 - val_accuracy: 0.6803 - val_loss: 0.7930\n",
            "Epoch 34/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6936 - loss: 0.7702 - val_accuracy: 0.7269 - val_loss: 0.7290\n",
            "Epoch 35/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7115 - loss: 0.7428 - val_accuracy: 0.7543 - val_loss: 0.6563\n",
            "Epoch 36/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7089 - loss: 0.7341 - val_accuracy: 0.5590 - val_loss: 1.1199\n",
            "Epoch 37/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.6801 - loss: 0.7955 - val_accuracy: 0.7631 - val_loss: 0.6457\n",
            "Epoch 38/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.7150 - loss: 0.7235 - val_accuracy: 0.7345 - val_loss: 0.7305\n",
            "Epoch 39/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.6929 - loss: 0.7676 - val_accuracy: 0.7694 - val_loss: 0.6604\n",
            "Epoch 40/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7084 - loss: 0.7222 - val_accuracy: 0.5906 - val_loss: 1.0055\n",
            "Epoch 41/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.6798 - loss: 0.7924 - val_accuracy: 0.6506 - val_loss: 0.8724\n",
            "Epoch 42/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.6967 - loss: 0.7596 - val_accuracy: 0.7358 - val_loss: 0.6619\n",
            "Epoch 43/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 36ms/step - accuracy: 0.7102 - loss: 0.7176 - val_accuracy: 0.6570 - val_loss: 0.8319\n",
            "Epoch 44/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7062 - loss: 0.7343 - val_accuracy: 0.7194 - val_loss: 0.7258\n",
            "Epoch 45/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.7086 - loss: 0.7256 - val_accuracy: 0.6114 - val_loss: 0.9439\n",
            "Epoch 46/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.6939 - loss: 0.7561 - val_accuracy: 0.6329 - val_loss: 0.9118\n",
            "Epoch 47/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7009 - loss: 0.7351 - val_accuracy: 0.7876 - val_loss: 0.5848\n",
            "Epoch 48/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.7244 - loss: 0.6878 - val_accuracy: 0.6265 - val_loss: 0.9186\n",
            "Epoch 49/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.6902 - loss: 0.7513 - val_accuracy: 0.7675 - val_loss: 0.6245\n",
            "Epoch 50/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.7149 - loss: 0.7115 - val_accuracy: 0.7856 - val_loss: 0.5916\n",
            "Epoch 51/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - accuracy: 0.7125 - loss: 0.7077 - val_accuracy: 0.7378 - val_loss: 0.6839\n",
            "Epoch 52/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7125 - loss: 0.7104 - val_accuracy: 0.7429 - val_loss: 0.6586\n",
            "Epoch 53/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.7283 - loss: 0.6814 - val_accuracy: 0.5124 - val_loss: 1.3247\n",
            "Epoch 54/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 38ms/step - accuracy: 0.7129 - loss: 0.7176 - val_accuracy: 0.7513 - val_loss: 0.6266\n",
            "Epoch 55/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7309 - loss: 0.6686 - val_accuracy: 0.3754 - val_loss: 1.7933\n",
            "Epoch 56/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - accuracy: 0.7087 - loss: 0.7347 - val_accuracy: 0.6722 - val_loss: 0.7969\n",
            "Epoch 57/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.7207 - loss: 0.6829 - val_accuracy: 0.7895 - val_loss: 0.5613\n",
            "Epoch 58/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.7270 - loss: 0.6718 - val_accuracy: 0.7890 - val_loss: 0.5666\n",
            "Epoch 59/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 46ms/step - accuracy: 0.7268 - loss: 0.6732 - val_accuracy: 0.6147 - val_loss: 0.9875\n",
            "Epoch 60/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - accuracy: 0.7339 - loss: 0.6545 - val_accuracy: 0.6407 - val_loss: 0.8530\n",
            "Epoch 61/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 54ms/step - accuracy: 0.7223 - loss: 0.6832 - val_accuracy: 0.6951 - val_loss: 0.7399\n",
            "Epoch 62/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 47ms/step - accuracy: 0.7267 - loss: 0.6754 - val_accuracy: 0.7704 - val_loss: 0.5878\n",
            "Epoch 63/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 47ms/step - accuracy: 0.7439 - loss: 0.6394 - val_accuracy: 0.7991 - val_loss: 0.5626\n",
            "Epoch 64/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 50ms/step - accuracy: 0.7493 - loss: 0.6222 - val_accuracy: 0.7518 - val_loss: 0.6157\n",
            "Epoch 65/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.7359 - loss: 0.6481 - val_accuracy: 0.7759 - val_loss: 0.6033\n",
            "Epoch 66/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 39ms/step - accuracy: 0.7360 - loss: 0.6594 - val_accuracy: 0.7353 - val_loss: 0.6511\n",
            "Epoch 67/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.7381 - loss: 0.6458 - val_accuracy: 0.7804 - val_loss: 0.5694\n",
            "Epoch 68/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.7285 - loss: 0.6648 - val_accuracy: 0.6912 - val_loss: 0.7540\n",
            "Epoch 69/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7415 - loss: 0.6339 - val_accuracy: 0.7932 - val_loss: 0.5422\n",
            "Epoch 70/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.7459 - loss: 0.6311 - val_accuracy: 0.7822 - val_loss: 0.5638\n",
            "Epoch 71/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 45ms/step - accuracy: 0.7334 - loss: 0.6546 - val_accuracy: 0.7323 - val_loss: 0.6629\n",
            "Epoch 72/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 38ms/step - accuracy: 0.7591 - loss: 0.6023 - val_accuracy: 0.7542 - val_loss: 0.6212\n",
            "Epoch 73/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.7514 - loss: 0.6190 - val_accuracy: 0.6899 - val_loss: 0.7427\n",
            "Epoch 74/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7341 - loss: 0.6447 - val_accuracy: 0.7705 - val_loss: 0.5770\n",
            "Epoch 75/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7482 - loss: 0.6249 - val_accuracy: 0.5293 - val_loss: 1.2443\n",
            "Epoch 76/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7268 - loss: 0.6640 - val_accuracy: 0.7920 - val_loss: 0.5682\n",
            "Epoch 77/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.7425 - loss: 0.6284 - val_accuracy: 0.6415 - val_loss: 0.8529\n",
            "Epoch 78/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 39ms/step - accuracy: 0.7464 - loss: 0.6279 - val_accuracy: 0.7879 - val_loss: 0.5513\n",
            "Epoch 79/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7526 - loss: 0.6097 - val_accuracy: 0.8005 - val_loss: 0.5301\n",
            "Epoch 80/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7512 - loss: 0.6018 - val_accuracy: 0.7808 - val_loss: 0.5573\n",
            "Epoch 81/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7486 - loss: 0.6211 - val_accuracy: 0.7492 - val_loss: 0.6260\n",
            "Epoch 82/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7497 - loss: 0.6198 - val_accuracy: 0.7954 - val_loss: 0.5273\n",
            "Epoch 83/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7485 - loss: 0.6144 - val_accuracy: 0.7502 - val_loss: 0.5999\n",
            "Epoch 84/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7562 - loss: 0.5931 - val_accuracy: 0.7636 - val_loss: 0.5914\n",
            "Epoch 85/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7358 - loss: 0.6361 - val_accuracy: 0.7980 - val_loss: 0.5384\n",
            "Epoch 86/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.7475 - loss: 0.6206 - val_accuracy: 0.6360 - val_loss: 0.9304\n",
            "Epoch 87/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7404 - loss: 0.6280 - val_accuracy: 0.7495 - val_loss: 0.6229\n",
            "Epoch 88/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7380 - loss: 0.6373 - val_accuracy: 0.6771 - val_loss: 0.7681\n",
            "Epoch 89/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7623 - loss: 0.5868 - val_accuracy: 0.7653 - val_loss: 0.5943\n",
            "Epoch 90/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7535 - loss: 0.5966 - val_accuracy: 0.7034 - val_loss: 0.7270\n",
            "Epoch 91/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7539 - loss: 0.5949 - val_accuracy: 0.7130 - val_loss: 0.7030\n",
            "Epoch 92/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7382 - loss: 0.6362 - val_accuracy: 0.7019 - val_loss: 0.7454\n",
            "Epoch 93/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7428 - loss: 0.6190 - val_accuracy: 0.8025 - val_loss: 0.5170\n",
            "Epoch 94/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7502 - loss: 0.6154 - val_accuracy: 0.7958 - val_loss: 0.5216\n",
            "Epoch 95/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7528 - loss: 0.6093 - val_accuracy: 0.6570 - val_loss: 0.8438\n",
            "Epoch 96/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7389 - loss: 0.6324 - val_accuracy: 0.8005 - val_loss: 0.5213\n",
            "Epoch 97/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7521 - loss: 0.6013 - val_accuracy: 0.6487 - val_loss: 0.8927\n",
            "Epoch 98/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7520 - loss: 0.6070 - val_accuracy: 0.6716 - val_loss: 0.8357\n",
            "Epoch 99/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.7564 - loss: 0.6004 - val_accuracy: 0.7715 - val_loss: 0.5673\n",
            "Epoch 100/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7509 - loss: 0.5976 - val_accuracy: 0.7410 - val_loss: 0.6298\n",
            "Epoch 101/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7581 - loss: 0.6016 - val_accuracy: 0.7882 - val_loss: 0.5420\n",
            "Epoch 102/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7442 - loss: 0.6248 - val_accuracy: 0.7251 - val_loss: 0.6634\n",
            "Epoch 103/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7652 - loss: 0.5727 - val_accuracy: 0.6207 - val_loss: 0.9669\n",
            "Epoch 104/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7417 - loss: 0.6318 - val_accuracy: 0.7912 - val_loss: 0.5277\n",
            "Epoch 105/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7659 - loss: 0.5706 - val_accuracy: 0.7841 - val_loss: 0.5500\n",
            "Epoch 106/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7622 - loss: 0.5819 - val_accuracy: 0.7398 - val_loss: 0.6313\n",
            "Epoch 107/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7630 - loss: 0.5744 - val_accuracy: 0.6203 - val_loss: 0.9569\n",
            "Epoch 108/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7543 - loss: 0.6023 - val_accuracy: 0.8022 - val_loss: 0.5190\n",
            "Epoch 109/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7575 - loss: 0.5915 - val_accuracy: 0.8032 - val_loss: 0.5016\n",
            "Epoch 110/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7645 - loss: 0.5659 - val_accuracy: 0.7391 - val_loss: 0.6367\n",
            "Epoch 111/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7648 - loss: 0.5734 - val_accuracy: 0.7895 - val_loss: 0.5289\n",
            "Epoch 112/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.7580 - loss: 0.5836 - val_accuracy: 0.8058 - val_loss: 0.5011\n",
            "Epoch 113/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 40ms/step - accuracy: 0.7633 - loss: 0.5852 - val_accuracy: 0.7890 - val_loss: 0.5333\n",
            "Epoch 114/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 46ms/step - accuracy: 0.7578 - loss: 0.5891 - val_accuracy: 0.8018 - val_loss: 0.5324\n",
            "Epoch 115/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.7643 - loss: 0.5729 - val_accuracy: 0.7811 - val_loss: 0.5596\n",
            "Epoch 116/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.7610 - loss: 0.5881 - val_accuracy: 0.7793 - val_loss: 0.5473\n",
            "Epoch 117/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7660 - loss: 0.5733 - val_accuracy: 0.7734 - val_loss: 0.5607\n",
            "Epoch 118/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7661 - loss: 0.5645 - val_accuracy: 0.7688 - val_loss: 0.5916\n",
            "Epoch 119/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7571 - loss: 0.5841 - val_accuracy: 0.7088 - val_loss: 0.7018\n",
            "Epoch 120/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.7704 - loss: 0.5600 - val_accuracy: 0.7215 - val_loss: 0.6689\n",
            "Epoch 121/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.7686 - loss: 0.5670 - val_accuracy: 0.7992 - val_loss: 0.5165\n",
            "Epoch 122/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7669 - loss: 0.5705 - val_accuracy: 0.8127 - val_loss: 0.4950\n",
            "Epoch 123/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7723 - loss: 0.5505 - val_accuracy: 0.7940 - val_loss: 0.5132\n",
            "Epoch 124/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 43ms/step - accuracy: 0.7673 - loss: 0.5627 - val_accuracy: 0.7056 - val_loss: 0.7259\n",
            "Epoch 125/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.7660 - loss: 0.5678 - val_accuracy: 0.7997 - val_loss: 0.5143\n",
            "Epoch 126/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 40ms/step - accuracy: 0.7726 - loss: 0.5550 - val_accuracy: 0.8116 - val_loss: 0.5025\n",
            "Epoch 127/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 41ms/step - accuracy: 0.7625 - loss: 0.5759 - val_accuracy: 0.7669 - val_loss: 0.5758\n",
            "Epoch 128/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.7614 - loss: 0.5820 - val_accuracy: 0.7874 - val_loss: 0.5314\n",
            "Epoch 129/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7674 - loss: 0.5619 - val_accuracy: 0.7958 - val_loss: 0.5083\n",
            "Epoch 130/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7750 - loss: 0.5512 - val_accuracy: 0.7221 - val_loss: 0.6636\n",
            "Epoch 131/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7594 - loss: 0.5858 - val_accuracy: 0.7691 - val_loss: 0.5753\n",
            "Epoch 132/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7666 - loss: 0.5613 - val_accuracy: 0.7643 - val_loss: 0.5683\n",
            "Epoch 133/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7789 - loss: 0.5355 - val_accuracy: 0.7903 - val_loss: 0.5596\n",
            "Epoch 134/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7656 - loss: 0.5725 - val_accuracy: 0.8072 - val_loss: 0.5060\n",
            "Epoch 135/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.7760 - loss: 0.5443 - val_accuracy: 0.7822 - val_loss: 0.5399\n",
            "Epoch 136/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.7660 - loss: 0.5624 - val_accuracy: 0.7611 - val_loss: 0.5805\n",
            "Epoch 137/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7631 - loss: 0.5624 - val_accuracy: 0.8085 - val_loss: 0.4937\n",
            "Epoch 138/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 46ms/step - accuracy: 0.7795 - loss: 0.5334 - val_accuracy: 0.7862 - val_loss: 0.5352\n",
            "Epoch 139/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.7541 - loss: 0.5984 - val_accuracy: 0.7889 - val_loss: 0.5273\n",
            "Epoch 140/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7766 - loss: 0.5441 - val_accuracy: 0.7538 - val_loss: 0.5992\n",
            "Epoch 141/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 48ms/step - accuracy: 0.7732 - loss: 0.5574 - val_accuracy: 0.7894 - val_loss: 0.5209\n",
            "Epoch 142/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7623 - loss: 0.5860 - val_accuracy: 0.8111 - val_loss: 0.4937\n",
            "Epoch 143/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7750 - loss: 0.5382 - val_accuracy: 0.7232 - val_loss: 0.6672\n",
            "Epoch 144/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7721 - loss: 0.5536 - val_accuracy: 0.7683 - val_loss: 0.5656\n",
            "Epoch 145/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.7664 - loss: 0.5588 - val_accuracy: 0.7818 - val_loss: 0.5397\n",
            "Epoch 146/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.7788 - loss: 0.5366 - val_accuracy: 0.7476 - val_loss: 0.6111\n",
            "Epoch 147/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - accuracy: 0.7602 - loss: 0.5814 - val_accuracy: 0.7617 - val_loss: 0.5682\n",
            "Epoch 148/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7670 - loss: 0.5642 - val_accuracy: 0.6540 - val_loss: 0.9107\n",
            "Epoch 149/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7735 - loss: 0.5612 - val_accuracy: 0.8001 - val_loss: 0.5328\n",
            "Epoch 150/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7798 - loss: 0.5351 - val_accuracy: 0.8129 - val_loss: 0.4827\n",
            "Epoch 151/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.7754 - loss: 0.5412 - val_accuracy: 0.6858 - val_loss: 0.7746\n",
            "Epoch 152/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.7549 - loss: 0.6040 - val_accuracy: 0.7694 - val_loss: 0.5789\n",
            "Epoch 153/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7751 - loss: 0.5458 - val_accuracy: 0.6412 - val_loss: 0.9410\n",
            "Epoch 154/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7728 - loss: 0.5578 - val_accuracy: 0.5992 - val_loss: 1.0653\n",
            "Epoch 155/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7701 - loss: 0.5641 - val_accuracy: 0.7695 - val_loss: 0.5624\n",
            "Epoch 156/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7735 - loss: 0.5423 - val_accuracy: 0.6892 - val_loss: 0.7735\n",
            "Epoch 157/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7804 - loss: 0.5355 - val_accuracy: 0.6331 - val_loss: 0.9712\n",
            "Epoch 158/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7661 - loss: 0.5666 - val_accuracy: 0.8129 - val_loss: 0.4931\n",
            "Epoch 159/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7744 - loss: 0.5512 - val_accuracy: 0.7841 - val_loss: 0.5471\n",
            "Epoch 160/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - accuracy: 0.7819 - loss: 0.5331 - val_accuracy: 0.8021 - val_loss: 0.5080\n",
            "Epoch 161/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7733 - loss: 0.5469 - val_accuracy: 0.8003 - val_loss: 0.4996\n",
            "Epoch 162/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 44ms/step - accuracy: 0.7690 - loss: 0.5495 - val_accuracy: 0.7892 - val_loss: 0.5164\n",
            "Epoch 163/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.7679 - loss: 0.5679 - val_accuracy: 0.7632 - val_loss: 0.5812\n",
            "Epoch 164/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 40ms/step - accuracy: 0.7685 - loss: 0.5551 - val_accuracy: 0.6857 - val_loss: 0.7604\n",
            "Epoch 165/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 37ms/step - accuracy: 0.7713 - loss: 0.5512 - val_accuracy: 0.7493 - val_loss: 0.6082\n",
            "Epoch 166/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.7660 - loss: 0.5546 - val_accuracy: 0.7988 - val_loss: 0.5043\n",
            "Epoch 167/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.7735 - loss: 0.5344 - val_accuracy: 0.8042 - val_loss: 0.4996\n",
            "Epoch 168/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7770 - loss: 0.5395 - val_accuracy: 0.7199 - val_loss: 0.6685\n",
            "Epoch 169/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.7788 - loss: 0.5344 - val_accuracy: 0.8037 - val_loss: 0.4950\n",
            "Epoch 170/300\n",
            "\u001b[1m381/381\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.7796 - loss: 0.5347 - val_accuracy: 0.8120 - val_loss: 0.4860\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7833 - loss: 0.5316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized LSTM Model (20 timesteps) - Test Loss: 0.5307, Test Accuracy: 0.7848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model prediction probabilities\n",
        "predictionscnn_pro = lstm_model.predict(matrices_4x4.reshape(-1, 20, 1))\n",
        "\n",
        "# Get predicted class labels\n",
        "predicted_classes = np.argmax(predictionscnn_pro, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-a473EWLFVZ",
        "outputId": "f7c0478a-066c-4a1c-cc1f-70090763060f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m18751/18751\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 9ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get the total number of classes\n",
        "num_classes = predictionscnn_pro.shape[1]\n",
        "\n",
        "# Initialize the final predictions array\n",
        "final_predictions = np.zeros(predictionscnn_pro.shape[0])  # Initialize to 0\n",
        "\n",
        "# Define the class(es) to be labeled as 1\n",
        "target_classes = {sorted_indices[-1]}  # Assume sorted_indices is predefined\n",
        "\n",
        "# Loop through each sample\n",
        "for i in range(predictionscnn_pro.shape[0]):\n",
        "    # Get the predicted class label for the current sample\n",
        "    class_label = predicted_classes[i]\n",
        "\n",
        "    # If the predicted class is in the target class set, mark as 1; otherwise, 0\n",
        "    final_predictions[i] = 1 if class_label in target_classes else 0\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(final_predictions == labels3)\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(labels3, final_predictions, labels=[0, 1])\n",
        "\n",
        "# Calculate Precision\n",
        "tp = cm[1, 1]  # True Positives\n",
        "fp = cm[0, 1]  # False Positives\n",
        "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "\n",
        "# Calculate Recall\n",
        "fn = cm[1, 0]  # False Negatives\n",
        "recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "\n",
        "# Calculate F1 Score\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
        "\n",
        "# Print results\n",
        "print(\"Final Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Precision (TP / (TP + FP)): {precision:.2%}\")\n",
        "print(f\"Recall (TP / (TP + FN)): {recall:.2%}\")\n",
        "print(f\"F1 Score: {f1:.2%}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPLJrnmKLQB1",
        "outputId": "2f3aa945-efd7-4f38-bc3c-bba5a18f579b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Results:\n",
            "Accuracy: 48.40%\n",
            "Precision (TP / (TP + FP)): 60.58%\n",
            "Recall (TP / (TP + FN)): 0.94%\n",
            "F1 Score: 1.84%\n",
            "Confusion Matrix:\n",
            "[[287501   1890]\n",
            " [307707   2905]]\n"
          ]
        }
      ]
    }
  ]
}